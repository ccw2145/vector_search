{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the catalog and schema to use. You must have USE_CATALOG privilege on the catalog and USE_SCHEMA and CREATE_TABLE privileges on the schema.\n",
    "# Change the catalog and schema here if necessary.\n",
    "dbutils.widgets.text(\"catalog\", \"main\")\n",
    "dbutils.widgets.text(\"schema_name\", \"default\")\n",
    "dbutils.widgets.text(\"source_data_path\", \"dbfs:/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet\")\n",
    "dbutils.widgets.text(\"source_table_name\",\"wiki_articles_demo\")\n",
    "\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "source_table_name = dbutils.widgets.get(\"source_table_name\")\n",
    "source_data_path = dbutils.widgets.get(\"source_data_path\")\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from vector_search.etl_utils import chunk_text\n",
    "\n",
    "\n",
    "source_table_fullname = f\"{catalog_name}.{schema_name}.{source_table_name}\"\n",
    "source_df = spark.read.parquet(source_data_path).limit(5)\n",
    "# The GTE model has been trained on a max context lenth of 8192 tokens.\n",
    "\n",
    "max_chunk_tokens = 500\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Process the data and store in a new list\n",
    "pandas_df = source_df.toPandas()\n",
    "processed_data = []\n",
    "for index, row in pandas_df.iterrows():\n",
    "    text_chunks = chunk_text(row['text'], max_chunk_tokens, encoding)\n",
    "    chunk_no = 0\n",
    "    for chunk in text_chunks:\n",
    "        row_data = row.to_dict()\n",
    "        \n",
    "        # replace the id column with a new unique chunk id\n",
    "        # and the text column with the text chunk\n",
    "        row_data['id'] = f\"{row['id']}_{chunk_no}\"\n",
    "        row_data['text'] = chunk\n",
    "        \n",
    "        processed_data.append(row_data)\n",
    "        chunk_no += 1\n",
    "\n",
    "chunked_pandas_df = pd.DataFrame(processed_data)\n",
    "chunked_spark_df = spark.createDataFrame(chunked_pandas_df)\n",
    "\n",
    "# Write the chunked DataFrame to a Delta table\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {source_table_fullname}\")\n",
    "chunked_spark_df.write.format(\"delta\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .saveAsTable(source_table_fullname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
